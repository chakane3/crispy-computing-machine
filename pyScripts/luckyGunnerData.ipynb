{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import json\n",
    "\n",
    "rimfire = \"https://www.luckygunner.com/rimfire\"\n",
    "handgun = \"https://www.luckygunner.com/handgun\"\n",
    "rifle = \"https://www.luckygunner.com/rifle\"\n",
    "shotgun = \"https://www.luckygunner.com/shotgun\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# returns 2 list of http links separated by availability\n",
    "def luckyGunner_availability(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 \\\n",
    "        (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "        \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    page_body = soup.body\n",
    "    products = soup.find_all('div', class_='main')\n",
    "\n",
    "    \n",
    "    # list of all handgun ammo seperated by availability\n",
    "    products = soup.find_all('div', class_='block-content block-no-padding')\n",
    "    bulletsList = str(products).split('<div class=\"main-category-cols\">')\n",
    "    inStock = bulletsList[1]\n",
    "    outofStock = bulletsList[2]\n",
    "\n",
    "    inStockScrapedLinks = []\n",
    "    outofStockScrapedLinks = []\n",
    "    \n",
    "    inStock = inStock.split(\"</li>\")\n",
    "    for inStockItems in inStock:\n",
    "        inStockScrapedLinks.append(re.findall(\"(?P<url>https?://[^\\s]+)\", str(inStockItems)))\n",
    "\n",
    "\n",
    "    outofStock = outofStock.split(\"</li>\")\n",
    "    for outofStockItems in outofStock:\n",
    "        outofStockScrapedLinks.append(re.findall(\"(?P<url>https?://[^\\s]+)\", str(outofStockItems)))\n",
    "\n",
    "    inStockCleanLinks = []\n",
    "    outofStockCleanLinks = []\n",
    "    \n",
    "\n",
    "    for i in inStockScrapedLinks:\n",
    "        cleanLink = \"\"\n",
    "        for j in i:\n",
    "            for k in j:\n",
    "                if k != '\"':\n",
    "                    cleanLink += k\n",
    "                    \n",
    "                else:\n",
    "                    inStockCleanLinks.append(cleanLink)\n",
    "                    break\n",
    "                \n",
    "    for i in outofStockScrapedLinks:\n",
    "        cleanLink = \"\"\n",
    "        for j in i:\n",
    "            for k in j:\n",
    "                if k != '\"':\n",
    "                    cleanLink += k\n",
    "                else:\n",
    "                    outofStockCleanLinks.append(cleanLink)\n",
    "                    break\n",
    "    return inStockCleanLinks, outofStockCleanLinks\n",
    "\n",
    "handGunAmmoInStock, handGunAmmoOutofStock = luckyGunner_availability(handgun)\n",
    "rifleAmmoInStock, rifleAmmoOutofStock = luckyGunner_availability(rifle)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scrapeHandGunAmmo(link):\n",
    "    link+=\"?limit=all\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 \\\n",
    "        (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "    html_response = requests.get(url=link, headers = headers)\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    page_body = soup.body\n",
    "    products = soup.find_all('div', class_='main')\n",
    "\n",
    "    names = []\n",
    "    prices = []\n",
    "\n",
    "    # list of html elements\n",
    "    names_list = soup.find_all(\"h3\")\n",
    "    prices_list = soup.find_all(\"div\", class_=\"price-box\")\n",
    "\n",
    "    \n",
    "    for i in prices_list:\n",
    "\n",
    "        # if \n",
    "        if \"Special Price\" in i:\n",
    "            sp = i.find(\"p\", class_=\"special-price\")\n",
    "            sp = sp.find(\"span\", class_=\"price\")\n",
    "            prices.append(sp.text)\n",
    "        else:\n",
    "            prices.append(i.text)\n",
    "\n",
    "\n",
    "    for i in  names_list:\n",
    "        names.append(i.text)\n",
    "\n",
    "    try:\n",
    "        ammoData = pd.DataFrame(\n",
    "            {\"name\": names, \"price\": prices}\n",
    "        )\n",
    "        return ammoData\n",
    "        \n",
    "    except:\n",
    "        print(\"Check link\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scrapeRifleAmmo(link):\n",
    "\n",
    "    link+=\"?limit=all\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 \\\n",
    "        (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "    html_response = requests.get(url=link, headers = headers)\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    page_body = soup.body    \n",
    "\n",
    "    prices = []\n",
    "    names = []\n",
    "\n",
    "    # list of html elements\n",
    "    names_list = soup.find_all(\"h3\")\n",
    "    prices_list = soup.find_all(\"div\", class_=\"price-box\")\n",
    "\n",
    "    \n",
    "    for i in prices_list:\n",
    "\n",
    "        # if \n",
    "        if \"Special Price\" in i:\n",
    "            sp = i.find(\"p\", class_=\"special-price\")\n",
    "            sp = sp.find(\"span\", class_=\"price\")\n",
    "            prices.append(sp.text)\n",
    "        else:\n",
    "            prices.append(i.text)\n",
    "\n",
    "\n",
    "    for i in  names_list:\n",
    "        names.append(i.text)\n",
    "\n",
    "    try:\n",
    "        ammoData = pd.DataFrame(\n",
    "            {\"name\": names, \"price\": prices}\n",
    "        )\n",
    "        return ammoData\n",
    "        \n",
    "    except:\n",
    "        print(\"Check link\")\n",
    "        \n",
    "\n",
    "    print(\"\\n\\n------------------------------------\\n\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scrapeShotgunAmmo(link):\n",
    "\n",
    "    link+=\"?limit=all\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 \\\n",
    "        (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "    html_response = requests.get(url=link, headers = headers)\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    page_body = soup.body    \n",
    "\n",
    "    prices = []\n",
    "    names = []\n",
    "    dirty_links = []\n",
    "    clean_links = []\n",
    "\n",
    "    # list of html elements\n",
    "    names_list = soup.find_all(\"h3\")\n",
    "    gauges = soup.find_all(\"ul\", class_=\"dropdown-menu\")\n",
    "\n",
    "    try:\n",
    "        for i in gauges[3]:\n",
    "            aLink = i.find(\"a\")\n",
    "            aLink = re.findall(\"(?P<url>https?://[^\\s]+)\", str(aLink))\n",
    "            if len(aLink) != 0:\n",
    "                dirty_links.append(aLink)\n",
    "            \n",
    "    except:\n",
    "        print(-1)\n",
    "\n",
    "    for i in dirty_links:\n",
    "        clean_links.append(i[0].strip('\"'))\n",
    "\n",
    "\n",
    "    return clean_links"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scrapeShotgunAmmo_2(link):\n",
    "\n",
    "    link+=\"?limit=all\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 \\\n",
    "        (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "    html_response = requests.get(url=link, headers = headers)\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    names = []\n",
    "    prices = []\n",
    "\n",
    "    name = soup.find_all(\"h3\")\n",
    "\n",
    "    \n",
    "    for i in name:\n",
    "        names.append(i.text)\n",
    "\n",
    "    prices_list = soup.find_all(\"div\", class_=\"price-box\")\n",
    "    for i in prices_list:\n",
    "        if \"Special Price\" in i:\n",
    "            sp = i.find(\"p\", class_=\"special-price\")\n",
    "            sp = sp.find(\"span\", class_=\"price\")\n",
    "            prices.append(sp.text)\n",
    "        else:\n",
    "            prices.append(i.text)\n",
    "\n",
    "    try:\n",
    "        ammoData = pd.DataFrame(\n",
    "            {\"name\": names, \"price\": prices}\n",
    "        )\n",
    "        return ammoData\n",
    "    except:\n",
    "        print(\"check link: \" + link)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scrapeRimfire(link):\n",
    "\n",
    "    link+=\"?limit=all\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 \\\n",
    "        (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "    html_response = requests.get(url=link, headers = headers)\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    names = []\n",
    "    prices = []\n",
    "    dirty_links = []\n",
    "    clean_links = []\n",
    "\n",
    "    caliber = soup.find_all(\"ul\", class_=\"dropdown-menu\")\n",
    "    for i in caliber[2]:\n",
    "        aLink = i.find(\"a\")\n",
    "        aLink = re.findall(\"(?P<url>https?://[^\\s]+)\", str(aLink))\n",
    "        if len(aLink) != 0:\n",
    "            dirty_links.append(aLink)\n",
    "        \n",
    "    for i in dirty_links:\n",
    "        aLink = i[0].strip('\"')\n",
    "        clean_links.append(aLink)\n",
    "        \n",
    "    return clean_links"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scrapeRimefireAmmo_2(link):\n",
    "    link+=\"?limit=all\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 \\\n",
    "        (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "    html_response = requests.get(url=link, headers = headers)\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "\n",
    "    names = []\n",
    "    prices = []\n",
    "    name = soup.find_all(\"h3\")\n",
    "    price = soup.find_all(\"div\", class_=\"price-box\")\n",
    "    for i in name:\n",
    "        names.append(i.text)\n",
    "    \n",
    "\n",
    "    for i in price:\n",
    "        if \"Special Price\" in i:\n",
    "            sp = i.find(\"p\", class_=\"special-price\")\n",
    "            sp = sp.find(\"span\", class_=\"price\")\n",
    "            prices.append(sp.text)\n",
    "        else:\n",
    "            prices.append(i.text)\n",
    "\n",
    "    try:\n",
    "        ammoData = pd.DataFrame(\n",
    "            {\"name\": names, \"price\": prices}\n",
    "        )\n",
    "        return ammoData\n",
    "    except:\n",
    "        print(\"check link: \" + link)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rimfireAmmo = []\n",
    "rimfire_links = scrapeRimfire(rimfire)\n",
    "for i in range(len(rimfire_links)-1):\n",
    "    df = scrapeRimefireAmmo_2(rimfire_links[i])\n",
    "    rimfireAmmo.append(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shotgun_ammo = []\n",
    "shotgun_links = scrapeShotgunAmmo(shotgun)\n",
    "for i in range(len(shotgun_links)-1):\n",
    "    shotgun_ammo.append(scrapeShotgunAmmo_2(shotgun_links[i]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "handgunAmmo = []\n",
    "for availableAmmoLink in handGunAmmoInStock:\n",
    "    df = scrapeHandGunAmmo(availableAmmoLink)\n",
    "    handgunAmmo.append(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rifleAmmo = []\n",
    "for availableAmmoLink in rifleAmmoInStock:\n",
    "    df = scrapeRifleAmmo(availableAmmoLink)\n",
    "    rifleAmmo.append(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os, shutil, glob\n",
    "jsonStr = \"\"\n",
    "jsonStr += \"[\"\n",
    "aCount = 0\n",
    "for i in rifleAmmo:\n",
    "    res = i.to_json(orient=\"split\")\n",
    "    parsed = json.loads(res)\n",
    "    file = json.dumps(parsed)  \n",
    "    jsonStr += str(file)\n",
    "    \n",
    "    if aCount == len(rifleAmmo)-1:\n",
    "        continue\n",
    "    else:\n",
    "        jsonStr += \",\"\n",
    "        aCount += 1\n",
    "\n",
    "jsonStr += \"]\"\n",
    "d = json.loads(jsonStr)\n",
    "\n",
    "savePath = \"/Users/chakaneshegog/Desktop/Ammo_web/webScrape/jsonData/LuckyGunner\"\n",
    "with open(os.path.join(savePath, \"LGRifleAmmo.json\"), \"w\") as fp:\n",
    "    json.dump(d, fp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jsonStr = \"\"\n",
    "jsonStr += \"[\"\n",
    "aCount = 0\n",
    "for i in handgunAmmo:\n",
    "    res = i.to_json(orient=\"split\")\n",
    "    parsed = json.loads(res)\n",
    "    file = json.dumps(parsed)  \n",
    "    jsonStr += str(file)\n",
    "    \n",
    "    if aCount == len(handgunAmmo)-1:\n",
    "        continue\n",
    "    else:\n",
    "        jsonStr += \",\"\n",
    "        aCount += 1\n",
    "\n",
    "jsonStr += \"]\"\n",
    "\n",
    "\n",
    "savePath = \"/Users/chakaneshegog/Desktop/Ammo_web/webScrape/jsonData/LuckyGunner\"\n",
    "with open(os.path.join(savePath, \"LGHandgunAmmo.json\"), \"w\") as fp:\n",
    "    json.dump(d, fp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jsonStr = \"\"\n",
    "jsonStr += \"[\"\n",
    "aCount = 0\n",
    "for i in shotgun_ammo:\n",
    "    res = i.to_json(orient=\"split\")\n",
    "    parsed = json.loads(res)\n",
    "    file = json.dumps(parsed)  \n",
    "    jsonStr += str(file)\n",
    "    \n",
    "    if aCount == len(shotgun_ammo)-1:\n",
    "        continue\n",
    "    else:\n",
    "        jsonStr += \",\"\n",
    "        aCount += 1\n",
    "\n",
    "jsonStr += \"]\"\n",
    "\n",
    "\n",
    "savePath = \"/Users/chakaneshegog/Desktop/Ammo_web/webScrape/jsonData/LuckyGunner\"\n",
    "with open(os.path.join(savePath, \"LGShotgunAmmo.json\"), \"w\") as fp:\n",
    "    json.dump(d, fp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jsonStr = \"\"\n",
    "jsonStr += \"[\"\n",
    "aCount = 0\n",
    "for i in rimfireAmmo:\n",
    "    res = i.to_json(orient=\"split\")\n",
    "    parsed = json.loads(res)\n",
    "    file = json.dumps(parsed)  \n",
    "    jsonStr += str(file)\n",
    "    \n",
    "    if aCount == len(rimfireAmmo)-1:\n",
    "        continue\n",
    "    else:\n",
    "        jsonStr += \",\"\n",
    "        aCount += 1\n",
    "\n",
    "jsonStr += \"]\"\n",
    "\n",
    "\n",
    "savePath = \"/Users/chakaneshegog/Desktop/Ammo_web/webScrape/jsonData/LuckyGunner\"\n",
    "with open(os.path.join(savePath, \"LGRimfireAmmo.json\"), \"w\") as fp:\n",
    "    json.dump(d, fp)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}